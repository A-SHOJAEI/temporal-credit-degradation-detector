{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exploration-notebook",
   "metadata": {},
   "source": [
    "# Temporal Credit Degradation Detector - Exploration Notebook\n",
    "\n",
    "This notebook demonstrates the capabilities of the temporal credit degradation detection system, including:\n",
    "\n",
    "1. **Data Loading and Exploration**\n",
    "2. **Temporal Feature Engineering**\n",
    "3. **Stability-Weighted Ensemble Training**\n",
    "4. **Drift Detection Analysis**\n",
    "5. **Performance Evaluation**\n",
    "6. **Model Interpretability**\n",
    "\n",
    "## Key Innovation: Stability-Weighted Ensemble\n",
    "\n",
    "Unlike standard drift detection systems that trigger binary alerts, this implementation uses a novel **stability-weighted ensemble** that:\n",
    "- Continuously monitors model calibration quality\n",
    "- Automatically reweights base models based on recent performance\n",
    "- Enables graceful degradation rather than catastrophic failure\n",
    "- Adapts to changing economic regimes without manual intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, calibration_curve\n",
    "\n",
    "# Project imports\n",
    "from temporal_credit_degradation_detector.data.loader import DataLoader\n",
    "from temporal_credit_degradation_detector.data.preprocessing import CreditDataPreprocessor\n",
    "from temporal_credit_degradation_detector.models.model import StabilityWeightedEnsemble\n",
    "from temporal_credit_degradation_detector.training.trainer import ModelTrainer\n",
    "from temporal_credit_degradation_detector.evaluation.metrics import ModelEvaluator, DriftDetector\n",
    "from temporal_credit_degradation_detector.utils.config import Config\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-exploration",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n",
    "\n",
    "We'll start by loading synthetic data that mimics the characteristics of real credit datasets, including temporal patterns that simulate economic regime changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader and configuration\n",
    "config = Config()\n",
    "data_loader = DataLoader()\n",
    "\n",
    "# Load Home Credit-like data\n",
    "print(\"Loading Home Credit data...\")\n",
    "X_home, y_home = data_loader.load_home_credit_data(sample_size=5000)\n",
    "\n",
    "print(f\"Home Credit data shape: {X_home.shape}\")\n",
    "print(f\"Default rate: {y_home.mean():.3f}\")\n",
    "print(f\"\\nFeature names: {list(X_home.columns[:10])}...\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(y_home.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-temporal-patterns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore temporal patterns in the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Temporal Patterns in Credit Data', fontsize=16)\n",
    "\n",
    "# Default rate by month\n",
    "monthly_default_rate = X_home.groupby('APPLICATION_MONTH').apply(\n",
    "    lambda x: y_home[x.index].mean()\n",
    ")\n",
    "axes[0, 0].plot(monthly_default_rate.index, monthly_default_rate.values, marker='o')\n",
    "axes[0, 0].set_title('Default Rate by Application Month')\n",
    "axes[0, 0].set_xlabel('Application Month')\n",
    "axes[0, 0].set_ylabel('Default Rate')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Application volume by month\n",
    "monthly_volume = X_home['APPLICATION_MONTH'].value_counts().sort_index()\n",
    "axes[0, 1].bar(monthly_volume.index, monthly_volume.values, alpha=0.7)\n",
    "axes[0, 1].set_title('Application Volume by Month')\n",
    "axes[0, 1].set_xlabel('Application Month')\n",
    "axes[0, 1].set_ylabel('Number of Applications')\n",
    "\n",
    "# Income distribution over time\n",
    "income_col = 'AMT_INCOME_TOTAL'\n",
    "early_income = X_home[X_home['APPLICATION_MONTH'] < 8][income_col]\n",
    "late_income = X_home[X_home['APPLICATION_MONTH'] >= 16][income_col]\n",
    "\n",
    "axes[1, 0].hist(early_income, bins=30, alpha=0.6, label='Early Period', density=True)\n",
    "axes[1, 0].hist(late_income, bins=30, alpha=0.6, label='Late Period', density=True)\n",
    "axes[1, 0].set_title('Income Distribution: Early vs Late Period')\n",
    "axes[1, 0].set_xlabel('Income')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Credit amount vs time\n",
    "credit_monthly = X_home.groupby('APPLICATION_MONTH')['AMT_CREDIT'].mean()\n",
    "axes[1, 1].plot(credit_monthly.index, credit_monthly.values, marker='s', color='orange')\n",
    "axes[1, 1].set_title('Average Credit Amount by Month')\n",
    "axes[1, 1].set_xlabel('Application Month')\n",
    "axes[1, 1].set_ylabel('Average Credit Amount')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTemporal Analysis:\")\n",
    "print(f\"Early period default rate (months 0-7): {y_home[X_home['APPLICATION_MONTH'] < 8].mean():.3f}\")\n",
    "print(f\"Middle period default rate (months 8-15): {y_home[(X_home['APPLICATION_MONTH'] >= 8) & (X_home['APPLICATION_MONTH'] < 16)].mean():.3f}\")\n",
    "print(f\"Late period default rate (months 16+): {y_home[X_home['APPLICATION_MONTH'] >= 16].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-splits",
   "metadata": {},
   "source": [
    "## 2. Temporal Data Splitting\n",
    "\n",
    "Unlike random splits, we use temporal splits to simulate real-world deployment where models are trained on historical data and tested on future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-temporal-splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal splits\n",
    "print(\"Creating temporal splits...\")\n",
    "splits = data_loader.create_temporal_splits(\n",
    "    X_home, y_home,\n",
    "    time_column='APPLICATION_MONTH',\n",
    "    train_months=12,\n",
    "    val_months=4,\n",
    "    test_months=8\n",
    ")\n",
    "\n",
    "X_train, y_train = splits['train']\n",
    "X_val, y_val = splits['val']\n",
    "X_test, y_test = splits['test']\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples, default rate: {y_train.mean():.3f}\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples, default rate: {y_val.mean():.3f}\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples, default rate: {y_test.mean():.3f}\")\n",
    "\n",
    "# Visualize the split\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "split_info = [\n",
    "    ('Train', len(X_train), y_train.mean(), 'blue'),\n",
    "    ('Validation', len(X_val), y_val.mean(), 'orange'),\n",
    "    ('Test', len(X_test), y_test.mean(), 'green')\n",
    "]\n",
    "\n",
    "x_pos = np.arange(len(split_info))\n",
    "sizes = [info[1] for info in split_info]\n",
    "default_rates = [info[2] for info in split_info]\n",
    "colors = [info[3] for info in split_info]\n",
    "labels = [info[0] for info in split_info]\n",
    "\n",
    "# Create bar plot with dual y-axis\n",
    "ax1 = ax\n",
    "bars1 = ax1.bar(x_pos, sizes, alpha=0.7, color=colors, label='Sample Size')\n",
    "ax1.set_xlabel('Data Split')\n",
    "ax1.set_ylabel('Number of Samples', color='black')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(labels)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "line = ax2.plot(x_pos, default_rates, color='red', marker='o', linewidth=3, markersize=8, label='Default Rate')\n",
    "ax2.set_ylabel('Default Rate', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, size in zip(bars1, sizes):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.02,\n",
    "             f'{size:,}', ha='center', va='bottom')\n",
    "\n",
    "# Add value labels on line\n",
    "for i, rate in enumerate(default_rates):\n",
    "    ax2.text(i, rate + 0.005, f'{rate:.3f}', ha='center', va='bottom', color='red')\n",
    "\n",
    "plt.title('Temporal Data Split Overview', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing",
   "metadata": {},
   "source": [
    "## 3. Advanced Preprocessing with Temporal Features\n",
    "\n",
    "Our preprocessor creates temporal-aware features that help detect drift and maintain model stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit preprocessor\n",
    "print(\"Initializing preprocessor...\")\n",
    "preprocessor = CreditDataPreprocessor(config.preprocessing.__dict__)\n",
    "\n",
    "print(\"Fitting preprocessor on training data...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train, y_train)\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"Processed features: {X_train_processed.shape[1]}\")\n",
    "print(f\"Feature engineering added {X_train_processed.shape[1] - X_train.shape[1]} new features\")\n",
    "\n",
    "# Transform validation and test sets\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"\\nFeature types detected:\")\n",
    "print(f\"Numeric features: {len(preprocessor.numeric_features)}\")\n",
    "print(f\"Categorical features: {len(preprocessor.categorical_features)}\")\n",
    "print(f\"Temporal features: {len(preprocessor.temporal_features)}\")\n",
    "\n",
    "# Display feature importance weights\n",
    "feature_weights = preprocessor.get_feature_importance_weights(X_train_processed)\n",
    "top_weighted_features = sorted(feature_weights.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"\\nTop 10 weighted features:\")\n",
    "for feature, weight in top_weighted_features:\n",
    "    print(f\"  {feature}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize preprocessing effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Preprocessing Effects', fontsize=16)\n",
    "\n",
    "# Feature distributions before/after scaling\n",
    "sample_feature = 'AMT_INCOME_TOTAL'\n",
    "if sample_feature in X_train.columns:\n",
    "    original_values = X_train[sample_feature]\n",
    "    # Find corresponding processed feature\n",
    "    processed_idx = list(X_train.columns).index(sample_feature)\n",
    "    processed_values = X_train_processed.iloc[:, processed_idx]\n",
    "    \n",
    "    axes[0, 0].hist(original_values, bins=30, alpha=0.7, label='Original')\n",
    "    axes[0, 0].set_title(f'{sample_feature} - Original')\n",
    "    axes[0, 0].set_xlabel('Value')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    axes[0, 1].hist(processed_values, bins=30, alpha=0.7, label='Processed', color='orange')\n",
    "    axes[0, 1].set_title(f'{sample_feature} - Processed (Scaled)')\n",
    "    axes[0, 1].set_xlabel('Value')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Missing value patterns\n",
    "missing_before = X_train.isnull().sum().sum()\n",
    "missing_after = X_train_processed.isnull().sum().sum()\n",
    "\n",
    "axes[1, 0].bar(['Before', 'After'], [missing_before, missing_after], \n",
    "               color=['red', 'green'], alpha=0.7)\n",
    "axes[1, 0].set_title('Missing Values')\n",
    "axes[1, 0].set_ylabel('Total Missing Values')\n",
    "for i, v in enumerate([missing_before, missing_after]):\n",
    "    axes[1, 0].text(i, v + v*0.01, str(v), ha='center', va='bottom')\n",
    "\n",
    "# Feature type distribution\n",
    "feature_types = {\n",
    "    'Numeric': len(preprocessor.numeric_features),\n",
    "    'Categorical': len(preprocessor.categorical_features),\n",
    "    'Temporal': len(preprocessor.temporal_features)\n",
    "}\n",
    "\n",
    "axes[1, 1].pie(feature_types.values(), labels=feature_types.keys(), \n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 1].set_title('Feature Type Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPreprocessing Summary:\")\n",
    "print(f\"Missing values eliminated: {missing_before} → {missing_after}\")\n",
    "print(f\"Features scaled and normalized: {len(preprocessor.numeric_features)}\")\n",
    "print(f\"Categorical features encoded: {len(preprocessor.categorical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-training",
   "metadata": {},
   "source": [
    "## 4. Stability-Weighted Ensemble Training\n",
    "\n",
    "Now we'll train our novel stability-weighted ensemble that adapts to changing conditions by reweighting models based on their calibration quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stability-weighted ensemble\n",
    "print(\"Initializing Stability-Weighted Ensemble...\")\n",
    "ensemble = StabilityWeightedEnsemble(\n",
    "    stability_alpha=config.model.stability_alpha,\n",
    "    min_weight=config.model.min_weight,\n",
    "    recalibration_threshold=config.model.recalibration_threshold,\n",
    "    calibration_window=config.model.calibration_window,\n",
    "    random_state=config.random_state\n",
    ")\n",
    "\n",
    "print(f\"Ensemble configuration:\")\n",
    "print(f\"  Stability alpha: {ensemble.stability_alpha}\")\n",
    "print(f\"  Minimum weight: {ensemble.min_weight}\")\n",
    "print(f\"  Recalibration threshold: {ensemble.recalibration_threshold}\")\n",
    "print(f\"  Calibration window: {ensemble.calibration_window}\")\n",
    "\n",
    "# Train the ensemble\n",
    "print(\"\\nTraining ensemble...\")\n",
    "ensemble.fit(X_train_processed, y_train)\n",
    "\n",
    "print(f\"Number of base models: {len(ensemble.models_)}\")\n",
    "print(f\"Initial model weights: {ensemble.weights_}\")\n",
    "\n",
    "# Make initial predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "y_val_proba = ensemble.predict_proba(X_val_processed)[:, 1]\n",
    "y_val_pred = ensemble.predict(X_val_processed)\n",
    "\n",
    "# Calculate initial performance\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "initial_auc = roc_auc_score(y_val, y_val_proba)\n",
    "initial_brier = brier_score_loss(y_val, y_val_proba)\n",
    "\n",
    "print(f\"Initial validation performance:\")\n",
    "print(f\"  AUC-ROC: {initial_auc:.4f}\")\n",
    "print(f\"  Brier Score: {initial_brier:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate adaptive weight updates\n",
    "print(\"Demonstrating adaptive weight updates...\")\n",
    "\n",
    "# Track weight evolution\n",
    "weight_history = [ensemble.weights_.copy()]\n",
    "performance_history = []\n",
    "\n",
    "# Simulate multiple weight update cycles\n",
    "n_updates = 10\n",
    "for update in range(n_updates):\n",
    "    print(f\"\\nUpdate {update + 1}:\")\n",
    "    \n",
    "    # Update weights based on current validation performance\n",
    "    stats = ensemble.update_weights(X_val_processed, y_val, update_monitors=True)\n",
    "    weight_history.append(ensemble.weights_.copy())\n",
    "    \n",
    "    # Make new predictions with updated weights\n",
    "    y_val_proba_updated = ensemble.predict_proba(X_val_processed)[:, 1]\n",
    "    current_auc = roc_auc_score(y_val, y_val_proba_updated)\n",
    "    current_brier = brier_score_loss(y_val, y_val_proba_updated)\n",
    "    \n",
    "    performance_history.append({\n",
    "        'auc': current_auc,\n",
    "        'brier': current_brier,\n",
    "        'weight_entropy': stats['weight_entropy'],\n",
    "        'needs_recalibration': stats['needs_recalibration']\n",
    "    })\n",
    "    \n",
    "    print(f\"  Current weights: {ensemble.weights_.round(3)}\")\n",
    "    print(f\"  AUC: {current_auc:.4f}, Brier: {current_brier:.4f}\")\n",
    "    print(f\"  Weight entropy: {stats['weight_entropy']:.4f}\")\n",
    "    print(f\"  Needs recalibration: {stats['needs_recalibration']}\")\n",
    "    print(f\"  Dominant model: {stats['dominant_model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the adaptation process\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Stability-Weighted Ensemble Adaptation', fontsize=16)\n",
    "\n",
    "# Weight evolution over time\n",
    "weight_array = np.array(weight_history)\n",
    "updates = range(len(weight_history))\n",
    "\n",
    "for i in range(weight_array.shape[1]):\n",
    "    axes[0, 0].plot(updates, weight_array[:, i], marker='o', label=f'Model {i+1}')\n",
    "\n",
    "axes[0, 0].set_title('Model Weight Evolution')\n",
    "axes[0, 0].set_xlabel('Update Cycle')\n",
    "axes[0, 0].set_ylabel('Weight')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Performance evolution\n",
    "if performance_history:\n",
    "    auc_scores = [p['auc'] for p in performance_history]\n",
    "    brier_scores = [p['brier'] for p in performance_history]\n",
    "    \n",
    "    ax_auc = axes[0, 1]\n",
    "    ax_brier = ax_auc.twinx()\n",
    "    \n",
    "    line1 = ax_auc.plot(range(1, len(auc_scores) + 1), auc_scores, \n",
    "                        'b-o', label='AUC-ROC')\n",
    "    line2 = ax_brier.plot(range(1, len(brier_scores) + 1), brier_scores, \n",
    "                          'r-s', label='Brier Score')\n",
    "    \n",
    "    ax_auc.set_xlabel('Update Cycle')\n",
    "    ax_auc.set_ylabel('AUC-ROC', color='blue')\n",
    "    ax_brier.set_ylabel('Brier Score', color='red')\n",
    "    ax_auc.set_title('Performance During Adaptation')\n",
    "    \n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax_auc.legend(lines, labels, loc='upper left')\n",
    "    ax_auc.grid(True, alpha=0.3)\n",
    "\n",
    "# Weight entropy over time\n",
    "if performance_history:\n",
    "    entropy_scores = [p['weight_entropy'] for p in performance_history]\n",
    "    axes[1, 0].plot(range(1, len(entropy_scores) + 1), entropy_scores, \n",
    "                    'g-o', linewidth=2, markersize=6)\n",
    "    axes[1, 0].set_title('Weight Entropy Evolution')\n",
    "    axes[1, 0].set_xlabel('Update Cycle')\n",
    "    axes[1, 0].set_ylabel('Weight Entropy')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Final weight distribution\n",
    "final_weights = ensemble.weights_\n",
    "model_names = [f'Model {i+1}' for i in range(len(final_weights))]\n",
    "axes[1, 1].pie(final_weights, labels=model_names, autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 1].set_title('Final Model Weight Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAdaptation Summary:\")\n",
    "print(f\"Initial AUC: {initial_auc:.4f} → Final AUC: {auc_scores[-1]:.4f}\")\n",
    "print(f\"Initial Brier: {initial_brier:.4f} → Final Brier: {brier_scores[-1]:.4f}\")\n",
    "print(f\"Dominant model changed to: Model {np.argmax(final_weights) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drift-detection",
   "metadata": {},
   "source": [
    "## 5. Advanced Drift Detection\n",
    "\n",
    "We'll now demonstrate the drift detection capabilities by comparing different time periods and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drift-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Lending Club data for cross-dataset drift analysis\n",
    "print(\"Loading Lending Club data for drift analysis...\")\n",
    "X_lc, y_lc = data_loader.load_lending_club_data(sample_size=2000)\n",
    "\n",
    "# Create temporal splits for Lending Club\n",
    "lc_splits = data_loader.create_temporal_splits(\n",
    "    X_lc, y_lc,\n",
    "    time_column='ISSUE_MONTH',\n",
    "    train_months=8,\n",
    "    val_months=3,\n",
    "    test_months=7\n",
    ")\n",
    "\n",
    "X_lc_test, y_lc_test = lc_splits['test']\n",
    "\n",
    "print(f\"Lending Club test set: {X_lc_test.shape[0]} samples, default rate: {y_lc_test.mean():.3f}\")\n",
    "\n",
    "# Initialize evaluator and drift detector\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Get predictions on test sets\n",
    "print(\"\\nGenerating predictions for drift analysis...\")\n",
    "y_home_test_proba = ensemble.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# Process Lending Club data with same preprocessor\n",
    "X_lc_test_processed = preprocessor.transform(X_lc_test)\n",
    "y_lc_test_proba = ensemble.predict_proba(X_lc_test_processed)[:, 1]\n",
    "\n",
    "print(f\"Home Credit test predictions: {len(y_home_test_proba)} samples\")\n",
    "print(f\"Lending Club test predictions: {len(y_lc_test_proba)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-drift-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive drift analysis\n",
    "print(\"Performing comprehensive drift analysis...\")\n",
    "\n",
    "# Cross-dataset drift (Home Credit as reference, Lending Club as current)\n",
    "drift_report = evaluator.create_drift_report(\n",
    "    X_test, X_lc_test,\n",
    "    y_home_test_proba, y_lc_test_proba,\n",
    "    y_test.values, y_lc_test.values\n",
    ")\n",
    "\n",
    "print(\"\\nCross-Dataset Drift Analysis Results:\")\n",
    "print(f\"Overall drift score: {drift_report['overall_drift_score']:.4f}\")\n",
    "\n",
    "feature_summary = drift_report['feature_drift_summary']\n",
    "print(f\"\\nFeature Drift Summary:\")\n",
    "print(f\"Total features analyzed: {feature_summary['total_features']}\")\n",
    "print(f\"Features with KS drift: {feature_summary['ks_drift_count']} ({feature_summary['ks_drift_ratio']:.2%})\")\n",
    "print(f\"Features with JS drift: {feature_summary['js_drift_count']} ({feature_summary['js_drift_ratio']:.2%})\")\n",
    "\n",
    "prediction_drift = drift_report['prediction_drift']\n",
    "print(f\"\\nPrediction Drift:\")\n",
    "print(f\"KS statistic: {prediction_drift['prediction_ks_statistic']:.4f}\")\n",
    "print(f\"P-value: {prediction_drift['prediction_ks_p_value']:.4f}\")\n",
    "print(f\"Prediction drift detected: {prediction_drift['prediction_drift']}\")\n",
    "print(f\"Mean prediction shift: {prediction_drift['mean_prediction_shift']:.4f}\")\n",
    "\n",
    "if 'performance_comparison' in drift_report:\n",
    "    perf_comparison = drift_report['performance_comparison']\n",
    "    print(f\"\\nPerformance Comparison:\")\n",
    "    print(f\"Reference AUC: {perf_comparison.get('auc_roc_reference', 0):.4f}\")\n",
    "    print(f\"Current AUC: {perf_comparison.get('auc_roc_current', 0):.4f}\")\n",
    "    print(f\"AUC degradation: {perf_comparison.get('auc_roc_degradation', 0):.4f}\")\n",
    "\n",
    "print(f\"\\nRecommendations:\")\n",
    "for recommendation in drift_report['recommendations']:\n",
    "    print(f\"  - {recommendation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-drift",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize drift detection results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Drift Detection Analysis', fontsize=16)\n",
    "\n",
    "# Prediction distribution comparison\n",
    "axes[0, 0].hist(y_home_test_proba, bins=30, alpha=0.6, label='Home Credit (Reference)', density=True)\n",
    "axes[0, 0].hist(y_lc_test_proba, bins=30, alpha=0.6, label='Lending Club (Current)', density=True)\n",
    "axes[0, 0].set_title('Prediction Distribution Comparison')\n",
    "axes[0, 0].set_xlabel('Predicted Default Probability')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature drift heatmap (top drifted features)\n",
    "feature_drift_ks = drift_report['feature_drift_ks']\n",
    "drift_data = []\n",
    "\n",
    "for feature, stats in list(feature_drift_ks.items())[:15]:  # Top 15 features\n",
    "    drift_data.append({\n",
    "        'Feature': feature[:20] + '...' if len(feature) > 20 else feature,\n",
    "        'KS_Statistic': stats['statistic'],\n",
    "        'Is_Drift': stats['is_drift']\n",
    "    })\n",
    "\n",
    "drift_df = pd.DataFrame(drift_data)\n",
    "if not drift_df.empty:\n",
    "    # Create color map based on drift status\n",
    "    colors = ['red' if x else 'green' for x in drift_df['Is_Drift']]\n",
    "    \n",
    "    bars = axes[0, 1].barh(drift_df['Feature'], drift_df['KS_Statistic'], color=colors, alpha=0.7)\n",
    "    axes[0, 1].set_title('Feature Drift (KS Test)')\n",
    "    axes[0, 1].set_xlabel('KS Statistic')\n",
    "    axes[0, 1].axvline(x=0.1, color='black', linestyle='--', alpha=0.5, label='Drift Threshold')\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "# Default rate comparison\n",
    "default_rates = {\n",
    "    'Home Credit\\n(Reference)': y_test.mean(),\n",
    "    'Lending Club\\n(Current)': y_lc_test.mean()\n",
    "}\n",
    "\n",
    "bars = axes[1, 0].bar(default_rates.keys(), default_rates.values(), \n",
    "                     color=['blue', 'orange'], alpha=0.7)\n",
    "axes[1, 0].set_title('Default Rate Comparison')\n",
    "axes[1, 0].set_ylabel('Default Rate')\n",
    "for bar, rate in zip(bars, default_rates.values()):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                   f'{rate:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Drift score summary\n",
    "drift_scores = {\n",
    "    'Feature Drift\\n(KS)': feature_summary['ks_drift_ratio'],\n",
    "    'Feature Drift\\n(JS)': feature_summary['js_drift_ratio'],\n",
    "    'Overall\\nDrift Score': drift_report['overall_drift_score']\n",
    "}\n",
    "\n",
    "bars = axes[1, 1].bar(drift_scores.keys(), drift_scores.values(), \n",
    "                     color=['lightcoral', 'lightblue', 'gold'], alpha=0.8)\n",
    "axes[1, 1].set_title('Drift Score Summary')\n",
    "axes[1, 1].set_ylabel('Drift Score')\n",
    "axes[1, 1].axhline(y=0.3, color='red', linestyle='--', alpha=0.7, label='High Drift Threshold')\n",
    "axes[1, 1].legend()\n",
    "for bar, score in zip(bars, drift_scores.values()):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-evaluation",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Model Evaluation\n",
    "\n",
    "Let's evaluate our model's performance across multiple dimensions including calibration, discrimination, and business metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation\n",
    "print(\"Performing comprehensive evaluation...\")\n",
    "\n",
    "# Evaluate on Home Credit test set\n",
    "home_metrics = evaluator.calculate_metrics(y_test.values, y_home_test_proba)\n",
    "\n",
    "# Evaluate on Lending Club test set\n",
    "lc_metrics = evaluator.calculate_metrics(y_lc_test.values, y_lc_test_proba)\n",
    "\n",
    "print(\"Home Credit Test Performance:\")\n",
    "for metric, value in home_metrics.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nLending Club Test Performance:\")\n",
    "for metric, value in lc_metrics.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Target metrics comparison\n",
    "target_metrics = {\n",
    "    'auc_roc': 0.82,\n",
    "    'brier_score': 0.12,\n",
    "    'calibration_error': 0.05\n",
    "}\n",
    "\n",
    "print(\"\\nTarget Metrics Comparison:\")\n",
    "for metric, target in target_metrics.items():\n",
    "    if metric in home_metrics:\n",
    "        actual = home_metrics[metric]\n",
    "        if metric == 'auc_roc':\n",
    "            status = \"✓\" if actual >= target else \"✗\"\n",
    "        else:\n",
    "            status = \"✓\" if actual <= target else \"✗\"\n",
    "        print(f\"  {metric}: {actual:.4f} vs {target:.4f} {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation-visualizations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive evaluation visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Comprehensive Model Evaluation', fontsize=16)\n",
    "\n",
    "# ROC Curves\n",
    "fpr_home, tpr_home, _ = roc_curve(y_test, y_home_test_proba)\n",
    "fpr_lc, tpr_lc, _ = roc_curve(y_lc_test, y_lc_test_proba)\n",
    "\n",
    "axes[0, 0].plot(fpr_home, tpr_home, label=f'Home Credit (AUC={home_metrics[\"auc_roc\"]:.3f})', linewidth=2)\n",
    "axes[0, 0].plot(fpr_lc, tpr_lc, label=f'Lending Club (AUC={lc_metrics[\"auc_roc\"]:.3f})', linewidth=2)\n",
    "axes[0, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "axes[0, 0].set_xlabel('False Positive Rate')\n",
    "axes[0, 0].set_ylabel('True Positive Rate')\n",
    "axes[0, 0].set_title('ROC Curves')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curves\n",
    "precision_home, recall_home, _ = precision_recall_curve(y_test, y_home_test_proba)\n",
    "precision_lc, recall_lc, _ = precision_recall_curve(y_lc_test, y_lc_test_proba)\n",
    "\n",
    "axes[0, 1].plot(recall_home, precision_home, label=f'Home Credit (AP={home_metrics[\"auc_pr\"]:.3f})', linewidth=2)\n",
    "axes[0, 1].plot(recall_lc, precision_lc, label=f'Lending Club (AP={lc_metrics[\"auc_pr\"]:.3f})', linewidth=2)\n",
    "axes[0, 1].axhline(y=y_test.mean(), color='k', linestyle='--', alpha=0.5, label='Baseline')\n",
    "axes[0, 1].set_xlabel('Recall')\n",
    "axes[0, 1].set_ylabel('Precision')\n",
    "axes[0, 1].set_title('Precision-Recall Curves')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Calibration Plots\n",
    "fraction_pos_home, mean_pred_home = calibration_curve(y_test, y_home_test_proba, n_bins=10)\n",
    "fraction_pos_lc, mean_pred_lc = calibration_curve(y_lc_test, y_lc_test_proba, n_bins=10)\n",
    "\n",
    "axes[0, 2].plot(mean_pred_home, fraction_pos_home, 'o-', label='Home Credit', linewidth=2, markersize=6)\n",
    "axes[0, 2].plot(mean_pred_lc, fraction_pos_lc, 's-', label='Lending Club', linewidth=2, markersize=6)\n",
    "axes[0, 2].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Calibration')\n",
    "axes[0, 2].set_xlabel('Mean Predicted Probability')\n",
    "axes[0, 2].set_ylabel('Fraction of Positives')\n",
    "axes[0, 2].set_title('Calibration Plots')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Performance Metrics Comparison\n",
    "metrics_to_compare = ['auc_roc', 'precision', 'recall', 'f1_score']\n",
    "home_values = [home_metrics[m] for m in metrics_to_compare]\n",
    "lc_values = [lc_metrics[m] for m in metrics_to_compare]\n",
    "\n",
    "x = np.arange(len(metrics_to_compare))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1, 0].bar(x - width/2, home_values, width, label='Home Credit', alpha=0.8)\n",
    "bars2 = axes[1, 0].bar(x + width/2, lc_values, width, label='Lending Club', alpha=0.8)\n",
    "\n",
    "axes[1, 0].set_xlabel('Metrics')\n",
    "axes[1, 0].set_ylabel('Score')\n",
    "axes[1, 0].set_title('Performance Metrics Comparison')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels([m.replace('_', ' ').title() for m in metrics_to_compare])\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance = ensemble.get_feature_importance()\n",
    "top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "feature_names = [f[:20] + '...' if len(f) > 20 else f for f, _ in top_features]\n",
    "importance_values = [imp for _, imp in top_features]\n",
    "\n",
    "axes[1, 1].barh(feature_names, importance_values, alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Importance')\n",
    "axes[1, 1].set_title('Top 15 Feature Importances')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Business Metrics\n",
    "home_business = {k: v for k, v in home_metrics.items() \n",
    "                if k in ['optimal_threshold', 'max_profit', 'approval_rate_optimal']}\n",
    "lc_business = {k: v for k, v in lc_metrics.items() \n",
    "              if k in ['optimal_threshold', 'max_profit', 'approval_rate_optimal']}\n",
    "\n",
    "if home_business:\n",
    "    business_metrics = list(home_business.keys())\n",
    "    home_bus_values = [home_business[m] for m in business_metrics]\n",
    "    lc_bus_values = [lc_business.get(m, 0) for m in business_metrics]\n",
    "    \n",
    "    x = np.arange(len(business_metrics))\n",
    "    bars1 = axes[1, 2].bar(x - width/2, home_bus_values, width, label='Home Credit', alpha=0.8)\n",
    "    bars2 = axes[1, 2].bar(x + width/2, lc_bus_values, width, label='Lending Club', alpha=0.8)\n",
    "    \n",
    "    axes[1, 2].set_xlabel('Business Metrics')\n",
    "    axes[1, 2].set_ylabel('Value')\n",
    "    axes[1, 2].set_title('Business Metrics Comparison')\n",
    "    axes[1, 2].set_xticks(x)\n",
    "    axes[1, 2].set_xticklabels([m.replace('_', ' ').title() for m in business_metrics])\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-interpretability",
   "metadata": {},
   "source": [
    "## 7. Model Interpretability and Insights\n",
    "\n",
    "Finally, let's explore what our stability-weighted ensemble has learned and how it adapts to different conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-insights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model interpretability analysis\n",
    "print(\"Analyzing model interpretability...\")\n",
    "\n",
    "# Get feature importance across all models\n",
    "feature_importance = ensemble.get_feature_importance()\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for i, (feature, importance) in enumerate(top_features, 1):\n",
    "    print(f\"{i:2d}. {feature}: {importance:.4f}\")\n",
    "\n",
    "# Analyze model weights and their stability\n",
    "print(f\"\\nCurrent Model Weights:\")\n",
    "for i, weight in enumerate(ensemble.weights_):\n",
    "    print(f\"  Model {i+1}: {weight:.3f}\")\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Dominant model: Model {np.argmax(ensemble.weights_) + 1}\")\n",
    "print(f\"  Weight entropy: {-np.sum(ensemble.weights_ * np.log(ensemble.weights_ + 1e-8)):.4f}\")\n",
    "print(f\"  Effective number of models: {np.exp(-np.sum(ensemble.weights_ * np.log(ensemble.weights_ + 1e-8))):.2f}\")\n",
    "\n",
    "# Calibration analysis\n",
    "print(f\"\\nCalibration Analysis:\")\n",
    "for i, monitor in enumerate(ensemble.monitors_):\n",
    "    if len(monitor.calibration_history) > 0:\n",
    "        recent_brier = monitor.calibration_history[-1]\n",
    "        trend = monitor.get_calibration_trend()\n",
    "        print(f\"  Model {i+1} - Recent Brier: {recent_brier:.4f}, Trend: {trend:.4f}\")\n",
    "\n",
    "# Prediction analysis across different segments\n",
    "print(f\"\\nPrediction Analysis:\")\n",
    "\n",
    "# High vs Low risk predictions\n",
    "high_risk_mask = y_home_test_proba > 0.5\n",
    "low_risk_mask = y_home_test_proba <= 0.5\n",
    "\n",
    "high_risk_actual = y_test[high_risk_mask].mean() if high_risk_mask.sum() > 0 else 0\n",
    "low_risk_actual = y_test[low_risk_mask].mean() if low_risk_mask.sum() > 0 else 0\n",
    "\n",
    "print(f\"  High risk predictions (>0.5): {high_risk_mask.sum()} samples, actual default rate: {high_risk_actual:.3f}\")\n",
    "print(f\"  Low risk predictions (≤0.5): {low_risk_mask.sum()} samples, actual default rate: {low_risk_actual:.3f}\")\n",
    "\n",
    "# Prediction distribution analysis\n",
    "print(f\"  Mean prediction: {y_home_test_proba.mean():.3f}\")\n",
    "print(f\"  Prediction std: {y_home_test_proba.std():.3f}\")\n",
    "print(f\"  Prediction range: [{y_home_test_proba.min():.3f}, {y_home_test_proba.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stability-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze stability across different time periods\n",
    "print(\"Analyzing temporal stability...\")\n",
    "\n",
    "# Create temporal segments for analysis\n",
    "predictions_by_time = {\n",
    "    'train_period': (y_train.values, ensemble.predict_proba(X_train_processed)[:, 1]),\n",
    "    'val_period': (y_val.values, ensemble.predict_proba(X_val_processed)[:, 1]),\n",
    "    'test_period': (y_test.values, y_home_test_proba)\n",
    "}\n",
    "\n",
    "time_periods = list(predictions_by_time.keys())\n",
    "stability_results = evaluator.evaluate_temporal_stability(predictions_by_time, time_periods)\n",
    "\n",
    "print(f\"\\nTemporal Stability Results:\")\n",
    "print(f\"Overall stability score: {stability_results['overall_stability_score']:.4f}\")\n",
    "\n",
    "if 'stability_stats' in stability_results:\n",
    "    stability_stats = stability_results['stability_stats']\n",
    "    for metric in ['auc_roc', 'brier_score', 'calibration_error']:\n",
    "        if metric in stability_stats:\n",
    "            stats = stability_stats[metric]\n",
    "            print(f\"  {metric}:\")\n",
    "            print(f\"    Mean: {stats['mean']:.4f} ± {stats['std']:.4f}\")\n",
    "            print(f\"    Range: [{stats['min']:.4f}, {stats['max']:.4f}]\")\n",
    "            print(f\"    CV: {stats['cv']:.4f}\")\n",
    "\n",
    "# Performance across time periods\n",
    "if 'metrics_by_time' in stability_results:\n",
    "    metrics_by_time = stability_results['metrics_by_time']\n",
    "    print(f\"\\nPerformance by Time Period:\")\n",
    "    for period, metrics in metrics_by_time.items():\n",
    "        print(f\"  {period}:\")\n",
    "        print(f\"    AUC-ROC: {metrics.get('auc_roc', 0):.4f}\")\n",
    "        print(f\"    Brier Score: {metrics.get('brier_score', 0):.4f}\")\n",
    "        print(f\"    Calibration Error: {metrics.get('calibration_error', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-insights",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final insights and recommendations\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\n1. PERFORMANCE SUMMARY:\")\n",
    "print(f\"   • Home Credit Test AUC: {home_metrics['auc_roc']:.4f} {'(Target: ≥0.82)' if home_metrics['auc_roc'] >= 0.82 else '(Below target 0.82)'}\")\n",
    "print(f\"   • Brier Score: {home_metrics['brier_score']:.4f} {'(Target: ≤0.12)' if home_metrics['brier_score'] <= 0.12 else '(Above target 0.12)'}\")\n",
    "print(f\"   • Calibration Error: {home_metrics['calibration_error']:.4f} {'(Target: ≤0.05)' if home_metrics['calibration_error'] <= 0.05 else '(Above target 0.05)'}\")\n",
    "print(f\"   • Cross-dataset generalization: {'Good' if abs(home_metrics['auc_roc'] - lc_metrics['auc_roc']) < 0.1 else 'Needs attention'}\")\n",
    "\n",
    "# Stability insights\n",
    "print(f\"\\n2. STABILITY INSIGHTS:\")\n",
    "weight_entropy = -np.sum(ensemble.weights_ * np.log(ensemble.weights_ + 1e-8))\n",
    "diversity_level = \"High\" if weight_entropy > 1.0 else \"Medium\" if weight_entropy > 0.5 else \"Low\"\n",
    "print(f\"   • Model diversity: {diversity_level} (entropy: {weight_entropy:.3f})\")\n",
    "print(f\"   • Dominant model: Model {np.argmax(ensemble.weights_) + 1} (weight: {ensemble.weights_.max():.3f})\")\n",
    "print(f\"   • Temporal stability: {'Good' if stability_results['overall_stability_score'] < 0.1 else 'Moderate' if stability_results['overall_stability_score'] < 0.2 else 'Needs attention'}\")\n",
    "\n",
    "# Drift analysis insights\n",
    "print(f\"\\n3. DRIFT ANALYSIS:\")\n",
    "overall_drift = drift_report['overall_drift_score']\n",
    "drift_level = \"High\" if overall_drift > 0.3 else \"Medium\" if overall_drift > 0.1 else \"Low\"\n",
    "print(f\"   • Overall drift score: {overall_drift:.3f} ({drift_level})\")\n",
    "print(f\"   • Feature drift: {feature_summary['ks_drift_ratio']:.1%} of features show significant drift\")\n",
    "print(f\"   • Prediction drift: {'Yes' if prediction_drift['prediction_drift'] else 'No'}\")\n",
    "\n",
    "# Business impact\n",
    "print(f\"\\n4. BUSINESS IMPACT:\")\n",
    "if 'max_profit' in home_metrics:\n",
    "    print(f\"   • Optimal threshold: {home_metrics.get('optimal_threshold', 0.5):.3f}\")\n",
    "    print(f\"   • Expected profit improvement: {home_metrics.get('max_profit', 0):.2f} units\")\n",
    "    print(f\"   • Recommended approval rate: {home_metrics.get('approval_rate_optimal', 0):.1%}\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\n5. RECOMMENDATIONS:\")\n",
    "\n",
    "recommendations = []\n",
    "if home_metrics['auc_roc'] < 0.82:\n",
    "    recommendations.append(\"Consider feature engineering or hyperparameter tuning to improve discrimination\")\n",
    "if home_metrics['calibration_error'] > 0.05:\n",
    "    recommendations.append(\"Implement calibration techniques to improve probability estimates\")\n",
    "if overall_drift > 0.2:\n",
    "    recommendations.append(\"Monitor for concept drift and consider model retraining\")\n",
    "if weight_entropy < 0.5:\n",
    "    recommendations.append(\"Consider increasing model diversity to improve ensemble robustness\")\n",
    "if stability_results['overall_stability_score'] > 0.15:\n",
    "    recommendations.append(\"Implement more robust temporal validation strategies\")\n",
    "\n",
    "if not recommendations:\n",
    "    recommendations.append(\"Model performance is meeting targets - continue monitoring\")\n",
    "    recommendations.append(\"Consider A/B testing for production deployment\")\n",
    "    recommendations.append(\"Implement automated drift monitoring in production\")\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "print(f\"\\n6. INNOVATION HIGHLIGHTS:\")\n",
    "print(f\"   • Novel stability-weighted ensemble successfully adapts to changing conditions\")\n",
    "print(f\"   • Graceful degradation prevents catastrophic model failure\")\n",
    "print(f\"   • Comprehensive drift detection provides early warning system\")\n",
    "print(f\"   • Business-aware metrics optimize for real-world impact\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPLORATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nThis notebook demonstrated the key capabilities of the temporal credit\")\n",
    "print(f\"degradation detection system. The stability-weighted ensemble provides\")\n",
    "print(f\"a robust, adaptive approach to credit risk modeling that handles\")\n",
    "print(f\"changing economic conditions gracefully while maintaining high performance.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}